// 1. å¯¼å…¥ä¾èµ–æ¨¡å—
import { tool } from "@langchain/core/tools";
import { HumanMessage, AIMessage, SystemMessage } from "@langchain/core/messages";
import { MemorySaver, StateGraph, MessagesAnnotation, START, END } from "@langchain/langgraph";
import { ToolNode } from "@langchain/langgraph/prebuilt";
import { ChatOpenAI } from "@langchain/openai";
import { z } from "zod";
import { LLM_API } from "../API_KEYS";

// 2. åˆå§‹åŒ– KIMI LLMï¼ˆä½¿ç”¨ OpenAI å…¼å®¹çš„æ¥å£ï¼‰
const llm = new ChatOpenAI({
  model: "moonshot-v1-8k",
  apiKey: LLM_API.KIMI_API_KEY,
  configuration: {
    baseURL: LLM_API.KIMI_API_BASE,
  },
  temperature: 0.7,
});

// 3. å®šä¹‰ç³»ç»Ÿæç¤ºï¼ˆAgentè§’è‰²ä¸è¡Œä¸ºè§„åˆ™ï¼‰
const systemPrompt = 
`ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¤©æ°”é¢„æŠ¥å‘˜ï¼Œå–œæ¬¢ä½¿ç”¨åŒå…³è¯­ã€‚

ä½ å¯ä»¥ä½¿ç”¨ä¸¤ä¸ªå·¥å…·ï¼š
- get_weather_for_location: ç”¨äºè·å–ç‰¹å®šä½ç½®çš„å¤©æ°”
- get_user_location: ç”¨äºè·å–ç”¨æˆ·çš„ä½ç½®

å¦‚æœç”¨æˆ·è¯¢é—®å¤©æ°”ï¼Œç¡®ä¿ä½ çŸ¥é“ä½ç½®ã€‚
å¦‚æœä»é—®é¢˜ä¸­å¯ä»¥çœ‹å‡ºä»–ä»¬æŒ‡çš„æ˜¯ä»–ä»¬æ‰€åœ¨çš„ä½ç½®ï¼Œä½¿ç”¨ get_user_location å·¥å…·æ¥æŸ¥æ‰¾ä»–ä»¬çš„ä½ç½®ã€‚`;

// 4. å®šä¹‰å·¥å…·ï¼ˆAgentå¯è°ƒç”¨çš„å‡½æ•°ï¼‰
// 4.1 å¤©æ°”æŸ¥è¯¢å·¥å…·ï¼ˆéœ€ä¼ å…¥åŸå¸‚åï¼‰
const getWeather = tool(
  async ({ city }) => {
    if(Math.random() > 0.5) {
        return `${city}çœ‹èµ·æ¥è¦ä¸‹é›¨äº†ï¼`;
    }
    return `${city}æ€»æ˜¯é˜³å…‰æ˜åªšï¼`;
  },
  {
    name: "get_weather_for_location",
    description: "è·å–æŒ‡å®šåŸå¸‚çš„å¤©æ°”",
    schema: z.object({
      city: z.string().describe("è¦æŸ¥è¯¢å¤©æ°”çš„åŸå¸‚"),
    }),
  }
);

// 4.2 è·å–ç”¨æˆ·ä½ç½®å·¥å…·
const getUserLocation = tool(
  async () => {
    // æ¨¡æ‹Ÿè·å–ç”¨æˆ·ä½ç½®
    return "åŒ—äº¬";
  },
  {
    name: "get_user_location",
    description: "è·å–ç”¨æˆ·å½“å‰æ‰€åœ¨çš„ä½ç½®",
    schema: z.object({}),
  }
);

// 5. ç»‘å®šå·¥å…·åˆ° LLM
const tools = [getWeather, getUserLocation];
const llmWithTools = llm.bindTools(tools);

// 6. å®šä¹‰ Agent èŠ‚ç‚¹
// 6.1 Agent èŠ‚ç‚¹ï¼šè°ƒç”¨ LLM
async function callModel(state: typeof MessagesAnnotation.State) {
  const messages = state.messages;
  const systemMessage = new SystemMessage(systemPrompt);
  const response = await llmWithTools.invoke([systemMessage, ...messages]);
  return { messages: [response] };
}

// 6.2 Tool èŠ‚ç‚¹ï¼šä½¿ç”¨ ToolNode æ‰§è¡Œå·¥å…·è°ƒç”¨
const toolNode = new ToolNode(tools);

// 7. å®šä¹‰è·¯ç”±å‡½æ•°ï¼šå†³å®šæ˜¯ç»§ç»­è°ƒç”¨å·¥å…·è¿˜æ˜¯ç»“æŸ
function shouldContinue(state: typeof MessagesAnnotation.State) {
  const messages = state.messages;
  const lastMessage = messages[messages.length - 1] as AIMessage;
  
  // å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œç»§ç»­æ‰§è¡Œå·¥å…·
  if (lastMessage.tool_calls && lastMessage.tool_calls.length > 0) {
    return "tools";
  }
  // å¦åˆ™ç»“æŸ
  return END;
}

// 8. æ„å»ºçŠ¶æ€å›¾ï¼ˆå·¥ä½œæµï¼‰
const workflow = new StateGraph(MessagesAnnotation)
  .addNode("agent", callModel)
  .addNode("tools", toolNode)
  .addEdge(START, "agent")
  .addConditionalEdges("agent", shouldContinue, {
    tools: "tools",
    [END]: END,
  })
  .addEdge("tools", "agent");

// 9. ç¼–è¯‘å›¾å¹¶æ·»åŠ è®°å¿†
const memory = new MemorySaver();
const app = workflow.compile({ checkpointer: memory });

// 10. ä¸»å‡½æ•°ï¼šè¿è¡Œ Agent
async function main() {
  console.log("ğŸ¤– KIMI LLM Agent å¯åŠ¨ä¸­...\n");
  
  // ç¤ºä¾‹å¯¹è¯
  const config = { configurable: { thread_id: "conversation-1" } };
  
  // ç¬¬ä¸€è½®å¯¹è¯
  console.log("ç”¨æˆ·ï¼šåŒ—äº¬çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ\n");
  let result = await app.invoke(
    { messages: [new HumanMessage("åŒ—äº¬çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ")] },
    config
  );
  
  const lastMessage = result.messages[result.messages.length - 1];
  console.log(`Agent: ${lastMessage.content}\n`);
  
  // ç¬¬äºŒè½®å¯¹è¯ï¼ˆæµ‹è¯•è®°å¿†åŠŸèƒ½ï¼‰
  console.log("ç”¨æˆ·ï¼šä¸Šæµ·å‘¢ï¼Ÿ\n");
  result = await app.invoke(
    { messages: [new HumanMessage("ä¸Šæµ·å‘¢ï¼Ÿ")] },
    config
  );
  
  const lastMessage2 = result.messages[result.messages.length - 1];
  console.log(`Agent: ${lastMessage2.content}\n`);
  
  console.log("âœ… å¯¹è¯å®Œæˆï¼");
}

// 11. è¿è¡Œ
main().catch(console.error);
